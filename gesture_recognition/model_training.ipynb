{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9f1669-3ed1-4c41-98c4-d491d3fe58eb",
   "metadata": {},
   "source": [
    "References: \n",
    "1. https://github.com/kinivi/tello-gesture-control\n",
    "2. https://www.youtube.com/watch?v=doDUihpj6ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89703192-2b09-49ee-b267-6b325ca9d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5dba14-a4ca-4272-abd6-2299abcde94a",
   "metadata": {},
   "source": [
    "# 1. Setting Up Mediapipe Hands Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5895e9c0-9af4-4121-a9f1-07d81eec0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc2074d-a9a1-4190-99b8-8145c651e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \n",
    "    # by default, the frame read by opencv is returned in BGR format instead of RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # make the image no longer writeable prior to detection\n",
    "    image.flags.writeable = False                  \n",
    "    results = model.process(image)                 \n",
    "    image.flags.writeable = True                   \n",
    "    \n",
    "    # convert back to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c6175b-7f07-44f4-8df0-f4bc960228c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the hand landmarks and connections in-place (will not return a new image)\n",
    "def draw_landmarks(image, results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "          mp_drawing.draw_landmarks(\n",
    "              image,\n",
    "              hand_landmarks,\n",
    "              mp_hands.HAND_CONNECTIONS,\n",
    "              mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "              mp_drawing_styles.get_default_hand_connections_style()\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ffa0e-213a-4396-bd77-944b1b8463e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Read Camera Feed with OpenCV & Perform Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfacd840-a3f4-4cd3-86d0-e9439e260d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# might need to change this if you have virtual devices set up for video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# set camera resolution to 1280x720\n",
    "# see https://stackoverflow.com/questions/11420748/setting-camera-parameters-in-opencv-python\n",
    "# 3. CV_CAP_PROP_FRAME_WIDTH Width of the frames in the video stream.\n",
    "# 4. CV_CAP_PROP_FRAME_HEIGHT Height of the frames in the video stream.\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7,\n",
    "        ) as hands:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # read the camera feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # mirror the frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        \n",
    "        draw_landmarks(image, results)\n",
    "\n",
    "        # show frame to user\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # if the q key is pressed, break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2353267-01c6-4f2b-9180-3caf83a0352e",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a51f914-01c9-4250-b59c-2d034c11095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single hand, there are 21 landmarks (see https://google.github.io/mediapipe/solutions/hands.html)\n",
    "# since we only care about x and y position of each landmark, we would need TOTAL_LANDMARKS * 2 points stored in the array for each hand\n",
    "TOTAL_LANDMARKS = 21\n",
    "TOTAL_POINTS = TOTAL_LANDMARKS * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf84f7e9-7437-4ef7-a069-a05cb96d6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input for the neural network\n",
    "def extract_keypoints(results):\n",
    "    \n",
    "    # initialise with all zeros\n",
    "    left_hand = np.zeros(TOTAL_POINTS)\n",
    "    right_hand = np.zeros(TOTAL_POINTS)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        \n",
    "        # NOTE: \n",
    "        \n",
    "        # handedness format with 2 hands,\n",
    "        # [classification {\n",
    "        #    index: 0\n",
    "        #    score: 0.9656149\n",
    "        #    label: \"Left\"\n",
    "        #  },\n",
    "        #  classification {\n",
    "        #    index: 1\n",
    "        #    score: 0.91598934\n",
    "        #    label: \"Right\"\n",
    "        #  }]\n",
    "        \n",
    "        # with only 1 hand (in this case, only a right hand),\n",
    "        # [classification {\n",
    "        #    index: 1\n",
    "        #    score: 0.9744842\n",
    "        #    label: \"Right\"\n",
    "        #  }]\n",
    "        \n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            \n",
    "            # if there is a left hand\n",
    "            if handedness.classification[0].index == 0:\n",
    "                left_hand = np.array([[landmark.x, landmark.y] for landmark in hand_landmarks.landmark]).flatten()\n",
    "                \n",
    "            # if there is a right hand\n",
    "            if handedness.classification[0].index == 1:\n",
    "                right_hand = np.array([[landmark.x, landmark.y] for landmark in hand_landmarks.landmark]).flatten()\n",
    "                \n",
    "    return np.concatenate([left_hand, right_hand])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb5696",
   "metadata": {},
   "source": [
    "## Extract keypoints only for middle finger MCP position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3726b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    # initialise with all zeros\n",
    "    # we only care about the middle finger MCP landmark, which contains x & y coordinates, so, 2 points total\n",
    "    left_hand = np.zeros(2)\n",
    "    right_hand = np.zeros(2)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            # if there is a left hand\n",
    "            if handedness.classification[0].index == 0:\n",
    "                left_hand = np.array([hand_landmarks.landmark[9].x, hand_landmarks.landmark[9].y])\n",
    "\n",
    "            # if there is a right hand\n",
    "            if handedness.classification[0].index == 1:\n",
    "                right_hand = np.array([hand_landmarks.landmark[9].x, hand_landmarks.landmark[9].y])\n",
    "    \n",
    "    return np.concatenate([left_hand, right_hand])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5664427-8e77-4e2c-a4af-c767d05f5da2",
   "metadata": {},
   "source": [
    "# 4. Initial Setup for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf64cdfa-c75f-4440-a0f2-dad230fc019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root folder for data collection\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# 'right swipe', 'left swipe', 'right swipe up', 'left swipe up', 'right swipe down', 'left swipe down', 'nogesture'\n",
    "actions = np.array(['right swipe', 'left swipe', 'right swipe up', 'left swipe up', 'nogesture'])\n",
    "\n",
    "# thirty videos to be recorded\n",
    "no_sequences = 10\n",
    "\n",
    "# videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# which action that we should start from?\n",
    "start_action_index = 0\n",
    "\n",
    "# from which point should we continue the data collection?\n",
    "# e.g. if there are already 30 videos, then we probably should start the data collection from video 31\n",
    "# if we want to overwrite the data that we have collected so far, we can start from 0\n",
    "start_folder = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9275ff9-d97e-4d65-ae4d-6dbc043d578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions[start_action_index:]: \n",
    "    \n",
    "    for sequence in range(start_folder, start_folder + no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            print(f'{os.path.join(DATA_PATH, action, str(sequence))} is already created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1af48-d235-44ed-b2a3-b08ebebf8158",
   "metadata": {},
   "source": [
    "# 5. Data Collection Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c7915d5-5f6c-4260-b7c9-35093b1e6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_flag = False\n",
    "\n",
    "# might need to change this if you have virtual devices set up for video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# set camera resolution to 1280x720\n",
    "# see https://stackoverflow.com/questions/11420748/setting-camera-parameters-in-opencv-python\n",
    "# 3. CV_CAP_PROP_FRAME_WIDTH Width of the frames in the video stream.\n",
    "# 4. CV_CAP_PROP_FRAME_HEIGHT Height of the frames in the video stream.\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7,\n",
    "        ) as hands:\n",
    "    for action in actions[start_action_index:]:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(start_folder, start_folder + no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            # the first frame, i.e. frame 0, will only be used to warn the user that the collection is starting\n",
    "            for frame_num in range(sequence_length + 1):\n",
    "\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                # mirror the frame\n",
    "                frame = cv2.flip(frame, 1)\n",
    "\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, hands)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_landmarks(image, results)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, f'STARTING COLLECTION FOR {action} Video Number {sequence}', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Collecting frames for {action} Video Number {sequence}', (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(1000)\n",
    "                else: \n",
    "                    cv2.putText(image, f'Collecting frames for {action} Video Number {sequence}', (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                    # NEW Export keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                    np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break_flag = True\n",
    "                    break\n",
    "                \n",
    "            if break_flag:\n",
    "                break\n",
    "        \n",
    "        if break_flag:\n",
    "                break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152a61d-bb67-47e1-a0c7-e0176fa9aac9",
   "metadata": {},
   "source": [
    "# 6. Create Labels & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cb7bc0-e90a-45ec-8ed7-6f431bd69d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = { label : num for num, label in enumerate(actions) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc233619-a4cf-41d8-90c2-8ac2b7f75937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right swipe': 0,\n",
       " 'left swipe': 1,\n",
       " 'right swipe up': 2,\n",
       " 'left swipe up': 3,\n",
       " 'nogesture': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e58b2e-4fbd-483d-b525-7ab5b379f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    # get all folder names from 0 to total folders\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        # create the video of length sequence_length\n",
    "        window = []\n",
    "        # starts from 1 since frame 0 was only used to warn the user that the data collection for the next video is starting\n",
    "        for frame_num in range(1, sequence_length+1):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), f\"{frame_num}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1f0150-4bf3-4ac7-9cd4-3dd024d880c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 30, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we have 5 gestures with 30 videos each,\n",
    "# each video has 30 frames,\n",
    "# each frame contains 2 hands\n",
    "# each hand contains 21 landmarks\n",
    "# each landmark contains 2 positions (x, y),\n",
    "# we have 150 videos of length 30 frames and each video contains 84 keypoints\n",
    "# (150, 30, 84)\n",
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b85f6306-361a-4ae4-9ab6-912d1652f828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc807c-cfe8-4187-a5a3-273ed5fbdac9",
   "metadata": {},
   "source": [
    "# 7. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c156fe6c-84da-4805-8491-4ea9714fed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9050a9e4-eeb8-477f-b6c9-32ea9bc354cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676beb6b-9a2b-4b0a-8b5d-1908f41170bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 30, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2b30363-86f1-457d-a760-54d30bc78c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d2b6ac7-094b-4e2d-abdf-844f66fc28e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4dffb8",
   "metadata": {},
   "source": [
    "## Preprocessing Method\n",
    "\n",
    "Reference: [https://github.com/kinivi/tello-gesture-control](https://github.com/kinivi/tello-gesture-control)\n",
    "\n",
    "Instead of feeding the raw output of the Mediapipe hands model immediately to our model, we preprocess the landmark by generating the position of each landmark relative to the first landmark (the palm of the hand).\n",
    "\n",
    "For instance, if the position of the palm is (1, 1), then for each landmark, we deduct its position (x, y) by (1, 1).\n",
    "\n",
    "Finally, we normalise each value by dividing each preprocessed landmark position with the maximum landmark value in a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5ef2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmark(frame):\n",
    "    new_frame = []\n",
    "    # base_x and base_y will be set to the position of the first non-zero landmark\n",
    "    # often, this should be the position of the left palm\n",
    "    # if there is no left palm detected, i.e. when the left palm position is (0,0), base_x and base_y will be set to the position of the right palm\n",
    "    # e.g. when the gesture is only using the right hand, such as right point or right pan\n",
    "    base_x, base_y = None, None\n",
    "    for index in range(0, frame.shape[0], 2):\n",
    "        # handle x and y\n",
    "        for point in range(index, index + 2):\n",
    "            new_point = frame[point]\n",
    "            # handle x\n",
    "            if point % 2 == 0:\n",
    "                # first non-zero x\n",
    "                if new_point != 0 and base_x is None:\n",
    "                    base_x = new_point\n",
    "                # 2nd, 3rd, etc. non-zero will be subtracted by base_x\n",
    "                elif new_point != 0 and base_x is not None:\n",
    "                    new_point -= base_x            \n",
    "            # handle y\n",
    "            else:\n",
    "                # first non-zero y\n",
    "                if new_point != 0 and base_y is None:\n",
    "                    base_y = new_point\n",
    "                # 2nd, 3rd, etc. non-zero will be subtracted by base_y\n",
    "                elif new_point != 0 and base_y is not None:\n",
    "                    new_point -= base_y\n",
    "            new_frame.append(new_point)\n",
    "        \n",
    "    # normalisation\n",
    "    max_value = max(list(map(abs, new_frame)))\n",
    "\n",
    "    if max_value != 0:\n",
    "\n",
    "        def normalize(n):\n",
    "            return n / max_value\n",
    "\n",
    "        new_frame = list(map(normalize, new_frame))\n",
    "        \n",
    "    return new_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144a47c",
   "metadata": {},
   "source": [
    "## With Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8a912ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = []\n",
    "\n",
    "for index, x in enumerate(X):\n",
    "    new_x = []\n",
    "    for frame in x:\n",
    "        new_x.append(preprocess_landmark(frame))\n",
    "\n",
    "    new_X.append(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55016a6",
   "metadata": {},
   "source": [
    "## Without Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87a8b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = []\n",
    "\n",
    "for index, x in enumerate(X):\n",
    "    new_x = []\n",
    "    for frame in x:\n",
    "        new_x.append(frame.tolist())\n",
    "\n",
    "    new_X.append(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0e9f8",
   "metadata": {},
   "source": [
    "## Output X and Y as JSON for Dynamic Time Warping Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c92f7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "FILE_NAME = \"../src/utilities/dataset.json\"\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for sequence, label in zip(new_X, labels):\n",
    "    datum = {\n",
    "        'label': label,\n",
    "        'sequence': sequence\n",
    "    }\n",
    "    dataset.append(datum)\n",
    "\n",
    "with open(FILE_NAME, 'w') as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9b3f48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.29173481,  1.        ,  0.00892439,\n",
       "       -0.11847133, -0.01740389, -0.1967694 , -0.0567551 , -0.24572852,\n",
       "       -0.0927835 , -0.26880913, -0.08825843, -0.19027201, -0.16152468,\n",
       "       -0.24151182, -0.20763462, -0.26910344, -0.24454299, -0.28615804,\n",
       "       -0.11309027, -0.12690495, -0.19417577, -0.18098953, -0.24831738,\n",
       "       -0.20995101, -0.28958496, -0.2313285 , -0.12730157, -0.05634325,\n",
       "       -0.20504173, -0.10825553, -0.25413735, -0.13518627, -0.29058299,\n",
       "       -0.15683176, -0.13074874,  0.01582301, -0.19662621, -0.01257965,\n",
       "       -0.23680725, -0.02943493, -0.26891472, -0.05045011])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X = np.array(new_X)\n",
    "new_X[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ecc052c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.19080925e-01,  8.27425778e-01,\n",
       "        2.19543189e-01,  7.06519008e-01,  1.80476487e-01,  6.30904138e-01,\n",
       "        1.32962376e-01,  6.02675378e-01,  9.93896276e-02,  5.99552870e-01,\n",
       "        1.70456409e-01,  6.05044961e-01,  1.11190714e-01,  5.21720171e-01,\n",
       "        7.37531930e-02,  4.67872739e-01,  4.44375351e-02,  4.24872637e-01,\n",
       "        1.39014542e-01,  6.49940133e-01,  7.21492916e-02,  5.45558274e-01,\n",
       "        2.83996463e-02,  4.81549859e-01, -2.68976390e-03,  4.32855308e-01,\n",
       "        1.09353565e-01,  7.04887569e-01,  4.80395928e-02,  6.05929434e-01,\n",
       "        9.83075052e-03,  5.48451364e-01, -1.72225609e-02,  5.02814353e-01,\n",
       "        8.63433555e-02,  7.68654644e-01,  3.02217826e-02,  6.99170113e-01,\n",
       "        4.14833426e-04,  6.63184285e-01, -2.32668743e-02,  6.28871143e-01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a7f9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6ba20c7-bebb-43b5-96e8-1221e15c21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dba9eaa0-bda4-431f-991c-0f92c4310ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026, 30, 84)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d20cf738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1026, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "102b05d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 30, 84)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1f26283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6c396-f22b-410c-bf82-60107c62572d",
   "metadata": {},
   "source": [
    "# 8. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8dfedfd-793f-41b0-9fa9-bb3ac4ef1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9946486-2a3a-4839-9739-9bf00e67a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "log_dir = os.path.join('logs', 'train', datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f234f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(patience=50, verbose=1, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4bfe867-b85d-4fe3-8597-caa33d5165a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(16, return_sequences=False, activation='relu', input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d0b29e0-dd31-49c2-885d-2d52dc8cf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f919f8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 16)                6464      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 6,636\n",
      "Trainable params: 6,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47f36ea4-25cf-47f1-bd79-c90013b7dc72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5/5 [==============================] - 4s 363ms/step - loss: 1.3883 - categorical_accuracy: 0.2526 - val_loss: 1.3382 - val_categorical_accuracy: 0.3333\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 1.3403 - categorical_accuracy: 0.2700 - val_loss: 1.2960 - val_categorical_accuracy: 0.4259\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 1.2971 - categorical_accuracy: 0.4590 - val_loss: 1.2513 - val_categorical_accuracy: 0.6481\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.2501 - categorical_accuracy: 0.6104 - val_loss: 1.1974 - val_categorical_accuracy: 0.7222\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.1965 - categorical_accuracy: 0.6848 - val_loss: 1.1259 - val_categorical_accuracy: 0.7593\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 1.1191 - categorical_accuracy: 0.7211 - val_loss: 1.0147 - val_categorical_accuracy: 0.9259\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 1.0115 - categorical_accuracy: 0.8503 - val_loss: 0.8571 - val_categorical_accuracy: 0.9815\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.8684 - categorical_accuracy: 0.9286 - val_loss: 0.7844 - val_categorical_accuracy: 0.8889\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.7125 - categorical_accuracy: 0.9424 - val_loss: 0.5664 - val_categorical_accuracy: 0.9630\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.5989 - categorical_accuracy: 0.9355 - val_loss: 0.4998 - val_categorical_accuracy: 0.9630\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.5156 - categorical_accuracy: 0.9867 - val_loss: 0.4441 - val_categorical_accuracy: 0.9815\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.4279 - categorical_accuracy: 0.9848 - val_loss: 0.3934 - val_categorical_accuracy: 0.9815\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.2776 - categorical_accuracy: 0.9898 - val_loss: 0.4117 - val_categorical_accuracy: 0.9630\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1978 - categorical_accuracy: 0.9934 - val_loss: 0.3797 - val_categorical_accuracy: 0.9630\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.1813 - categorical_accuracy: 0.9872 - val_loss: 0.4211 - val_categorical_accuracy: 0.9630\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.1238 - categorical_accuracy: 0.9907 - val_loss: 0.4794 - val_categorical_accuracy: 0.9630\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.1668 - categorical_accuracy: 0.9934 - val_loss: 0.4399 - val_categorical_accuracy: 0.9815\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.1553 - categorical_accuracy: 0.9889 - val_loss: 0.4716 - val_categorical_accuracy: 0.9630\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.1433 - categorical_accuracy: 0.9938 - val_loss: 0.4464 - val_categorical_accuracy: 0.9815\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0794 - categorical_accuracy: 0.9978 - val_loss: 0.4949 - val_categorical_accuracy: 0.9815\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.1114 - categorical_accuracy: 0.9969 - val_loss: 0.5615 - val_categorical_accuracy: 0.9815\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0543 - categorical_accuracy: 0.9995 - val_loss: 0.6331 - val_categorical_accuracy: 0.9815\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0542 - categorical_accuracy: 0.9993 - val_loss: 0.7368 - val_categorical_accuracy: 0.9630\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0619 - categorical_accuracy: 0.9985 - val_loss: 0.8507 - val_categorical_accuracy: 0.9630\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0353 - categorical_accuracy: 0.9978 - val_loss: 0.9372 - val_categorical_accuracy: 0.9630\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0384 - categorical_accuracy: 0.9976 - val_loss: 1.0072 - val_categorical_accuracy: 0.9630\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0770 - categorical_accuracy: 0.9966 - val_loss: 1.0329 - val_categorical_accuracy: 0.9630\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0708 - categorical_accuracy: 0.9983 - val_loss: 1.0515 - val_categorical_accuracy: 0.9630\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0322 - categorical_accuracy: 0.9993 - val_loss: 1.0677 - val_categorical_accuracy: 0.9630\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.0328 - categorical_accuracy: 0.9993 - val_loss: 1.1002 - val_categorical_accuracy: 0.9815\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0420 - categorical_accuracy: 0.9985 - val_loss: 0.9214 - val_categorical_accuracy: 0.9815\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.0501 - categorical_accuracy: 0.9983 - val_loss: 0.7032 - val_categorical_accuracy: 0.9815\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0241 - categorical_accuracy: 0.9995 - val_loss: 0.5825 - val_categorical_accuracy: 0.9815\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0559 - categorical_accuracy: 0.9976 - val_loss: 0.5460 - val_categorical_accuracy: 0.9815\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0246 - categorical_accuracy: 0.9995 - val_loss: 0.5527 - val_categorical_accuracy: 0.9815\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0237 - categorical_accuracy: 0.9995 - val_loss: 0.5771 - val_categorical_accuracy: 0.9815\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0490 - categorical_accuracy: 0.9983 - val_loss: 0.6093 - val_categorical_accuracy: 0.9815\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0223 - categorical_accuracy: 0.9993 - val_loss: 0.6467 - val_categorical_accuracy: 0.9815\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0408 - categorical_accuracy: 0.9983 - val_loss: 0.6842 - val_categorical_accuracy: 0.9815\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0246 - categorical_accuracy: 0.9990 - val_loss: 0.7156 - val_categorical_accuracy: 0.9815\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0227 - categorical_accuracy: 0.9990 - val_loss: 0.7430 - val_categorical_accuracy: 0.9815\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0132 - categorical_accuracy: 0.9995 - val_loss: 0.7713 - val_categorical_accuracy: 0.9815\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0101 - categorical_accuracy: 0.9995 - val_loss: 0.8019 - val_categorical_accuracy: 0.9815\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0167 - categorical_accuracy: 0.9978 - val_loss: 0.8200 - val_categorical_accuracy: 0.9815\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0180 - categorical_accuracy: 0.9983 - val_loss: 0.8205 - val_categorical_accuracy: 0.9815\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.0108 - categorical_accuracy: 0.9995 - val_loss: 0.8404 - val_categorical_accuracy: 0.9815\n",
      "Epoch 47/1000\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0123 - categorical_accuracy: 0.9983 - val_loss: 0.8798 - val_categorical_accuracy: 0.9815\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.0088 - categorical_accuracy: 0.9988 - val_loss: 0.8851 - val_categorical_accuracy: 0.9815\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0101 - categorical_accuracy: 0.9995 - val_loss: 0.8830 - val_categorical_accuracy: 0.9815\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0171 - categorical_accuracy: 0.9983 - val_loss: 0.8829 - val_categorical_accuracy: 0.9815\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.0057 - categorical_accuracy: 0.9995 - val_loss: 0.8924 - val_categorical_accuracy: 0.9815\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0065 - categorical_accuracy: 0.9993 - val_loss: 0.9113 - val_categorical_accuracy: 0.9815\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0077 - categorical_accuracy: 0.9978 - val_loss: 0.9219 - val_categorical_accuracy: 0.9815\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0084 - categorical_accuracy: 0.9993 - val_loss: 0.9232 - val_categorical_accuracy: 0.9815\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.0063 - categorical_accuracy: 0.9995 - val_loss: 0.9245 - val_categorical_accuracy: 0.9815\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.0051 - categorical_accuracy: 0.9995 - val_loss: 0.9332 - val_categorical_accuracy: 0.9815\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.0068 - categorical_accuracy: 0.9978 - val_loss: 0.9676 - val_categorical_accuracy: 0.9815\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0065 - categorical_accuracy: 0.9993 - val_loss: 0.9628 - val_categorical_accuracy: 0.9815\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0055 - categorical_accuracy: 0.9985 - val_loss: 0.9696 - val_categorical_accuracy: 0.9815\n",
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.0034 - categorical_accuracy: 0.9993 - val_loss: 0.9737 - val_categorical_accuracy: 0.9815\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0064 - categorical_accuracy: 0.9983 - val_loss: 0.9837 - val_categorical_accuracy: 0.9815\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0034 - categorical_accuracy: 0.9993 - val_loss: 0.9943 - val_categorical_accuracy: 0.9815\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.0034 - categorical_accuracy: 0.9995 - val_loss: 1.0107 - val_categorical_accuracy: 0.9815\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0034 - categorical_accuracy: 0.9985 - val_loss: 1.0297 - val_categorical_accuracy: 0.9815\n",
      "Epoch 00064: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x249991d1f40>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=256, epochs=1000, validation_data=(X_test, y_test), callbacks=[tb_callback, es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d158b-87ab-4284-8052-e6681c0d736a",
   "metadata": {},
   "source": [
    "# 9. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b031c0d-02db-4c36-b77c-a58a4b9b0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "935de3ae-1a5c-4c4c-9a46-f92295f580f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right finger snap'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "037387bf-85e7-4f12-8772-8969722d27ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right finger snap'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfbd68d3-e033-4641-8d95-7d90daa6c0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3727b-37f2-432e-8316-f32160d27136",
   "metadata": {},
   "source": [
    "# 10. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65d0340a-279a-4743-8c10-18b17fbecd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models is already created\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = os.path.join('models') \n",
    "\n",
    "try: \n",
    "    os.makedirs(os.path.join(MODEL_PATH))\n",
    "except:\n",
    "    print(f'{os.path.join(MODEL_PATH)} is already created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41eeab8a-ce15-4a14-a05f-1c01c7d2fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.h5'\n",
    "model.save(os.path.join(MODEL_PATH, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba5189f4-a5fb-4d1b-afeb-773f641f7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after running this cell, run the previous model definition cell\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b028d42-2d6d-4c67-9039-b9c2d0ca7b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(MODEL_PATH, '20220131-155214.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1296e-e5a3-4136-9eb3-409aecffa7e8",
   "metadata": {},
   "source": [
    "# 11. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e1f7b8f-125c-4a80-b2b5-b6b791404e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20307d79-5599-46b0-b3a9-5a1218fe9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e26af68-6ae6-496e-9873-f82a7a95b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd619e45-8a4e-4c86-98aa-7143a6e545ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[40,  0],\n",
       "        [ 1, 13]],\n",
       "\n",
       "       [[43,  1],\n",
       "        [ 0, 10]],\n",
       "\n",
       "       [[37,  0],\n",
       "        [ 0, 17]],\n",
       "\n",
       "       [[41,  0],\n",
       "        [ 0, 13]]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8e75193-48a2-41f9-bb41-71ff96b71151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814814814814815"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc8dad-ad79-4417-a6ce-27bcc3047a4e",
   "metadata": {},
   "source": [
    "# 12. Real-time Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3cf977a-23b3-4f03-8515-ce0cd8ee5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to change the number of colors when you change the total number of labels\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245), (117,245,16)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f13ffe3-372e-46d0-82f6-a7bd61f5244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the input needs to be 30 frames long, we need to store the frames into sequence array and only pass it to our model once it reaches 30 frames\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.9\n",
    "\n",
    "# might need to change this if you have virtual devices set up for video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# set camera resolution to 1280x720\n",
    "# see https://stackoverflow.com/questions/11420748/setting-camera-parameters-in-opencv-python\n",
    "# 3. CV_CAP_PROP_FRAME_WIDTH Width of the frames in the video stream.\n",
    "# 4. CV_CAP_PROP_FRAME_HEIGHT Height of the frames in the video stream.\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.7,\n",
    "        ) as hands:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # read the camera feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # mirror the frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        \n",
    "        draw_landmarks(image, results)\n",
    "\n",
    "        # append the new frame & grab the last 30 frames\n",
    "        keypoints = extract_keypoints(results)\n",
    "        keypoints = preprocess_landmark(keypoints)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        # if already 30 frames, start predicting\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            values, counts = np.unique(predictions[-15:], return_counts=True)\n",
    "            max_index = np.argmax(counts)\n",
    "            if values[max_index]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "\n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07103b67",
   "metadata": {},
   "source": [
    "# 13. Convert Keras model to TF.js Layers format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba8f13db-15a9-4f58-83dc-40bd86b739cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 15:53:26.640282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter --input_format keras \"models/20220131-155214.h5\" \"../public/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a281e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7277aee21dce51ed06e4ead48ac76498d4761481c88071b55336092cdfe8993"
  },
  "kernelspec": {
   "display_name": "Python [conda env:hansroslinger]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
